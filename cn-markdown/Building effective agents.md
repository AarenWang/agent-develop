过去一年，我们与各行各业数十个团队一起构建大型语言模型（LLM）智能体。最成功的实现并非依赖复杂框架或专用库，而是使用**简单且可组合的模式**。

本文汇总了我们在与客户共建和自研智能体过程中得到的经验，为开发者提供可落地的实践建议。

## 什么是智能体（agent）？

“Agent”在业内有多种定义：

* 有些客户将其视为**长时间自主运行**、利用多种工具完成复杂任务的系统；
* 也有人用它描述**遵循既定工作流**的实现。

在 Anthropic，我们将这些形态统称为 **agentic systems（智能体化系统）**，但会在架构上区分：

* **Workflows（工作流）**：LLM 与工具沿预定义的代码路径协同；
* **Agents（智能体）**：LLM 自主规划流程与工具调用，主动掌控完成任务的方式。

后文将深入探讨这两类系统，并在附录 1 的“Agents in Practice”中给出两个高价值场景。

## 何时（不）需要构建智能体

构建 LLM 应用时，优先选择最简单的方案，只有在必要时才增加复杂度——有时甚至不必做智能体。智能体常以更高的**延迟**和**成本**换取更好的任务表现，需评估这个权衡是否合理。

当确实需要更多复杂性时，**工作流**适合可预测、目标明确的任务；**智能体**更适合需要灵活度、依赖模型决策的大规模场景。对许多应用而言，**单次 LLM 调用 + 检索 + 上下文示例**就足够。

## 何时以及如何使用框架

以下框架可以降低构建智能体化系统的门槛：

* [Claude Agent SDK](https://platform.claude.com/docs/en/agent-sdk/overview)
* [Strands Agents SDK by AWS](https://strandsagents.com/latest/)
* [Rivet](https://rivet.ironcladapp.com/)（拖拽式 GUI LLM 工作流构建器）
* [Vellum](https://www.vellum.ai/)（用于构建、测试复杂工作流的 GUI 工具）

它们简化了调用 LLM、定义/解析工具、串联调用等底层任务，但也可能：

* 增加抽象层，遮蔽底层提示词与响应，调试更难；
* 让人过早叠加复杂度，而忽视更简洁的实现。

我们的建议：**先直接调用 LLM API**，很多模式只需几行代码即可实现；如果使用框架，务必弄清底层代码，错误的假设是客户问题的高发源。更多示例可参考 [cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents)。

## 构建模块、工作流与智能体

本节从基础模块（增强版 LLM）开始，逐步上升到组合式工作流和自治智能体，覆盖我们在生产环境中观察到的常见模式。

### 构建模块：增强版 LLM

增强版 LLM 是智能体化系统的基本单元，可叠加 **检索（retrieval）**、**工具调用（Tool Use）**、**记忆（memory）** 等能力。当前模型能主动生成检索查询、选择合适工具并决定保留哪些信息。实现时请关注两点：

1. 根据具体场景定制这些能力；
2. 为 LLM 提供**易用、文档清晰的接口**。

一种实现方式是使用 [Model Context Protocol (MCP, 模型上下文协议)](https://www.anthropic.com/news/model-context-protocol)，通过简单的[客户端实现](https://modelcontextprotocol.io/tutorials/building-a-client#building-mcp-clients)整合第三方工具生态。

### 工作流：提示词串联（prompt chaining）

提示词串联将任务分解为一系列步骤，每次 LLM 调用都会处理上一步的输出，可在中间步骤添加程序化校验（如下图中的“gate”）。

**适用场景：**

* **数据提取**：如将 PDF → 结构化 JSON；
* **执行追踪**：将误差溯源到具体步骤；
* **提示词调优**：针对单个环节独立优化；
* **可控输出**：使用模板对输出格式化；
* **按需检索**：仅在需要时查询数据库。

**优点：**可在本地验证、迭代每个步骤，调试开销小、延迟低。

**缺点：**链路长度受上下文限制；任意单点失败或漂移都可能破坏整体正确性；仅基于提示的输出解析可能不稳定。

> 【深度解析】提示词串联通过“可验证的小步快跑”降低风险，适合需要稳定产出的场景，如结构化抽取或规则化生成。对开发者而言，关键在于为每个节点设计可自动检查的“门控”，以便快速定位问题并提升可靠性。

### 工作流：提示词树形思维（prompt tree-of-thought）

树形思维是对串联的扩展：**并行**探索多个候选推理路径，然后挑选最优答案。它能提升准确率并支持更长的推理深度。

**适用场景：**

* **代码生成**：多版本候选供测试选择；
* **内容生成**：先并行生成摘要/标题/创意，再让模型筛选最佳方案。

**优点：**更健壮，能在更长上下文中维持高质量输出。

**缺点：**延迟和成本更高；需要额外模块来排名/选择最佳输出（如 reranker、额外 LLM 调用）。

> 【深度解析】树形思维通过“多样性 + 选择”提高鲁棒性，常与自动评估或投票机制结合使用。开发者应权衡并行分支带来的成本，并为“选择器”设计清晰的判据（测试结果、评分标准等）。

### 智能体：LLM 主导的循环

在增强版 LLM 与工作流基础上，可以构建**自治式智能体**。它们通过循环调度 LLM、工具调用、评估与记忆来完成开放任务。

**典型组件：**

* **短期记忆**：运行时的 scratchpad（如内联笔记、向量检索）；
* **长期记忆**：跨会话的持久存储（如摘要日志、历史对话）；
* **工具调用**：外部 API、数据库、文件/代码操作等；
* **评估与控制**：自动或人工检查点，避免失控。

**常见挑战**

1. **上下文飘移**：模型推理与指令可能逐步偏离，需在每次循环重新提供明确目标与进度；
2. **执行停滞**：智能体可能困在无效策略中，可通过**执行监督器**实时分析日志、调整工具参数，或引入人工审查；
3. **运行成本**：循环过长会放大费用与延迟，应根据任务价值设定迭代/时长上限；
4. **准确性退化**：长回合后输出质量降低，可通过**检索强化**、状态重置等手段缓解；
5. **工具设计不当**：命名含糊或接口复杂会导致错误调用；工具越**符合直觉**，模型表现越好。

为此，建议在工作流基础上逐步加入自治能力，保持可控范围与清晰的观察信号。

> 【深度解析】自治智能体的核心在于“感知—决策—行动”循环。开发者应提供足够的**外部反馈**（工具结果、测试、日志）让模型自我修正，并通过检查点或迭代上限限制自由度，从而在灵活性与安全性之间取得平衡。

### 人类在环（human-in-the-loop）

当智能体执行高风险或少见操作时，可引入人工审批：

* **自动建议，人工决策**：模型生成结果或操作草案，由人审核后执行；
* **主动寻求澄清**：模型在需要更多信息时请求人类输入；
* **按阈值触发审查**：若置信度低或差异大，则升级为人工评估；
* **关键事件审计**：对少见或高影响操作进行审核。

为了兼顾体验与安全性：

* 仅在必要时让用户介入，避免增加无谓负担；
* 让用户明确自己的决策空间，减少不确定性；
* 保持信息透明，帮助用户理解智能体的状态与风险。

> 【深度解析】人类在环机制是一种“软防护栏”，能在模型不确定或风险聚集时提供人工兜底。设计时应确保审查点明确、易用，并配合日志记录，方便后续迭代。

### 可靠性最佳实践

#### 1. 构建自动代理（autoregressive）心智模型

在提示词中明确期望行为，例如：

* **状态追踪**：让模型实时记账：当前目标、已尝试步骤、下一步计划；
* **输出格式化**：使用 JSON 模板、表格或编号，方便程序解析。

#### 2. 通过环境反馈改善性能

在推理链中加入 **事实来源**、**工具返回值**、**执行日志**，让模型基于“地面真实”判断进度。

#### 3. 精心设计 Agent-Computer Interface (ACI, 智能体-计算机接口)

* 用详尽的工具描述、示例和边界条件降低误用风险；
* 优化参数命名，使模型一眼能理解；
* 在沙箱中大量测试工具调用，迭代改进；
* 使用 **poka-yoke（防错设计）**，如要求绝对路径、防止危险操作。

#### 4. 使用外部评分器自动评测

可将模型作为“批评者”，对智能体输出进行质量打分。若某类错误可被约束规则定义，也可直接加入解析/检查逻辑。

#### 5. 加强评估覆盖率率

智能体的状态空间巨大，传统单次 LLM 测评不够。更有效的做法：

* **脚本化探针**：预设输入组合批量跑；
* **可重复场景**：固定上下文、工具结果，方便回归；
* **自动红队**：构建系统性攻击/压力测试以发现缺口。

> 【深度解析】可靠性建设的关键是“闭环反馈”：从提示、工具、测试到日志，形成可自动运行的评估流水线。这样才能在模型或工具更新时快速回归，维持质量。

#### 6. 对权衡做成本估算

* **总成本 = 每次迭代费用 × 平均迭代次数**；
* 控制复杂度（减少分支/循环）、减少工具调用、优化提示长度都有助于降低成本；
* 判断是否值得：便宜但低质的调用可能压低总体价值。

> 【深度解析】在自治循环中，迭代次数常是主要成本来源。开发者应设定清晰的停机条件（迭代上限、时间上限、质量阈值），并用 A/B 测试评估不同策略的“性价比”。

## 附录 1：生产中的智能体

### A. 客服自动化

客服场景天然适合智能体：

* 对话式流程与检索、动作执行结合紧密；
* 可通过工具获取客户数据、订单、知识库；
* 退款、工单更新等动作可程序化执行；
* 成功度量清晰（如一次性解决率）。

有些公司采用“按成功解决计费”的模式，进一步证明效果。

### B. 编码智能体

软件开发展现出巨大潜力，能力从补全演进到自主解题。智能体在此表现突出，因为：

* 代码结果可用自动化测试验证；
* 可依据测试反馈迭代；
* 问题空间明确、结构化；
* 质量可客观衡量。

我们的实现已能仅凭 PR 描述解决 [SWE-bench Verified](https://www.anthropic.com/research/swe-bench-sonnet) 基准中的真实 GitHub issue。尽管自动测试验证功能正确性，**人工评审**仍是保障系统契合业务需求的关键。

## 附录 2：为工具做提示工程

无论哪种智能体化系统，**工具**都是关键组成。Claude 在响应中会包含 **tool use block（工具调用块）** 表示计划调用的工具。工具定义与提示词同等重要，本节提供实用建议：

* 给予模型充分“思考”空间，避免写到死角；
* 选择模型在互联网上常见、易书写的格式；
* 减少格式开销，如避免要求精确行号或复杂转义；
* 将 HCI 的精力同等投入到 **agent-computer interface (ACI)**，例如：
  * 描述是否足够直观？是否包含示例、边界、输入格式？
  * 参数命名能否更清晰？
  * 在 [workbench](https://console.anthropic.com/workbench) 中批量测试，找出误用再迭代；
  * 通过 **poka-yoke**（防错）限制错误输入。

我们在构建 [SWE-bench](https://www.anthropic.com/research/swe-bench-sonnet) 智能体时发现，工具优化投入甚至超过提示本身。例如，模型在目录切换后容易用相对路径出错；要求工具始终使用绝对路径后，问题迎刃而解。

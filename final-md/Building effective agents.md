# Building effective agents｜学习指南

## 核心速览 (TL;DR)

* **先简后繁**：多数需求用单次 LLM + 检索就能满足，智能体（agent）只在确有灵活性收益时上场。
* **小步快跑的组合模式**：从增强版 LLM 起步，按需叠加 **prompt chaining**、树形思维、工具与记忆，再逐步开放自治循环。
* **可靠性靠接口与反馈**：精心打造 **agent-computer interface (ACI)**，把工具文档、测试和红队评估纳入闭环，必要时引入 **human-in-the-loop** 审核。

## 1. Agentic 系统地图

* **Agentic systems（智能体化系统）**涵盖两类：
  * **Workflows（工作流）**：按预定义路径 orchestrate LLM 与工具，重在可预测与一致性。
  * **Agents（智能体）**：LLM 自主规划步骤与 **Tool Use（工具调用）**，在开放场景中保持控制权。
* 取舍原则：先用最简单方案，确认需要灵活决策或多步推理时，再切换到智能体。

> 官方建议：在成本与延迟敏感的场景，优先用单次调用 + 检索；只在性能收益明显时才启用复杂工作流或自治智能体。

## 2. 何时采用框架

* 可用框架：**Claude Agent SDK**、**Strands Agents SDK**、**Rivet**、**Vellum**。
* 选择要点：
  * 先直接调用 API，很多模式几行代码即可；
  * 若使用框架，务必理解其提示词、工具解析、调用链细节，避免被额外抽象层遮蔽。

> 官方建议：无论是否用框架，都要看得见底层提示与响应，错误假设是常见故障源。

## 3. 从模块到工作流

### 3.1 构建块：增强版 LLM

* 在 Claude 上叠加 **检索（retrieval）**、**Tool Use**、**记忆（memory）**。
* 通过 **Model Context Protocol (MCP, 模型上下文协议)** 接入第三方工具生态，保持接口统一。
* 关键：接口必须易懂、示例充分，减少模型误用。

### 3.2 工作流模式

**Prompt chaining（提示词串联）**

* 任务拆解为连续步骤，可在节点加入“gate”校验。
* 适用：PDF→JSON 抽取、逐步调优、按需检索。
* 优缺点：调试轻量、延迟低；但链长受上下文限制，单点失败会连锁影响。

> 【深度解析】提示词串联强调“可验证的小步快跑”。为每个节点设计自动检查（格式、规则、简单测试），能迅速定位错误并提高稳定性。

**Prompt tree-of-thought（树形思维）**

* 并行生成多条推理路径，再用 reranker/LLM 选择最佳输出。
* 适用：代码生成多版本对比、创意/标题/摘要多样性探索。
* 优缺点：鲁棒、可延展长推理；但增加延迟与成本，需要额外的选择模块。

> 【深度解析】树形思维用“多样性 + 选择”换取质量。需要事先定义选择标准（测试结果、评分维度），并评估并行分支的成本收益。

## 4. 迈向自治智能体

### 4.1 典型架构

* **短期记忆**：scratchpad、向量检索，记录当前回合进展。
* **长期记忆**：摘要日志或历史会话，支撑跨任务延续性。
* **工具层**：API、数据库、文件/代码操作，暴露清晰参数与边界。
* **控制层**：自动/人工检查点，防止失控。

### 4.2 常见风险与缓解

* **上下文飘移**：在每轮循环重申目标与进度，必要时重置状态。
* **执行停滞**：引入执行监督器或人工审查，及时调整策略/参数。
* **成本膨胀**：设置迭代/时长上限，评估每步价值。
* **准确性退化**：用检索强化、分段重启维持质量。
* **工具设计不佳**：名称、约束、示例要直观；模型越容易“看懂”接口，表现越好。

> 【深度解析】自治循环的核心是“感知—决策—行动”。把工具结果、测试反馈、日志都喂回模型，让它依据真实环境自校准，再用检查点与上限收束自由度。

### 4.3 Human-in-the-loop

* 触发条件：高风险操作、置信度低、输出分歧大、关键事件审计。
* 交互方式：模型给出草案，人类批准；或模型主动请求澄清。
* 体验原则：仅在必要时打扰用户，明确可控范围，透明呈现风险与状态。

> 【深度解析】人类在环是“软防护栏”。清晰的升级路径 + 完整日志，能在风险积累时及时刹车，并为后续迭代提供证据。

## 5. 可靠性与评估清单

### 5.1 提示与接口

* 在提示里显式要求**状态追踪**（目标、已尝试、下一步）。
* 使用 JSON/表格等稳定格式，方便程序解析。
* 打磨 **ACI**：参数命名清晰、示例覆盖边界、禁止危险操作（如强制绝对路径）。

### 5.2 反馈与评分

* 把**事实来源、工具返回、执行日志**并入推理，形成可验证链路。
* 让模型或独立评分器对输出打分；对可规则化的错误用解析/校验替代额外推理。

### 5.3 覆盖率与红队

* 构建脚本化探针，批量跑固定场景，便于回归。
* 设计自动红队，系统性触发极端/误用输入，找出薄弱点。

### 5.4 成本权衡

* 估算：**总成本 = 每次迭代费用 × 平均迭代次数**。
* 控制复杂度（减少分支/循环）、减少工具调用、精简提示长度。
* 设定停机条件（迭代/时间上限、质量阈值），用 A/B 对比“性价比”。

> 官方建议：在 SWE-bench 智能体实践中，优化工具定义往往比改提示更划算；例如强制使用绝对路径即可消除大量调用错误。

## 6. 场景示例

### 客服自动化

* 对话流 + 检索 + 工具（查订单、退费、改工单）。
* 成功指标清晰（一次性解决率），甚至可按成功付费。

### 编码智能体

* 依赖自动化测试作为反馈，问题空间结构化、可客观评分。
* Anthropic 内部代理已能仅凭 PR 描述解决 **SWE-bench Verified** 真实 issue，但**人工 Code Review**仍不可或缺。

## 7. 下一步行动计划

1. 在项目中挑一个低风险任务，先用 **prompt chaining** 实现最小可行版本。
2. 按需接入 **Tool Use**，并用 **ACI** 原则补全参数说明、示例和防错约束。
3. 搭建脚本化评测与红队用例，固定场景便于回归；同时引入简单评分器监控质量。
4. 当需要自治时，再加入循环与记忆，并设定迭代/时间上限和人工审查触发条件。
5. 尝试 MCP 客户端或现有框架（如 Claude Agent SDK），评估是否能在保持可见性的前提下降低集成成本。
